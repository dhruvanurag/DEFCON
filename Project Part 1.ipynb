{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Complete\n",
      "[0.47606423 0.76180892]\n",
      "DHRUV\n",
      "[0.52658488 0.70408927]\n",
      "DHRUV\n",
      "[0.50955578 0.75989281]\n",
      "DHRUV\n",
      "[0.46268806 0.73553056]\n",
      "DHRUV\n",
      "[0.52826516 0.83731137]\n",
      "DHRUV\n",
      "[0.52262198 0.80894804]\n",
      "DHRUV\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "#importing necessary libraries/modules\n",
    "\n",
    "import cv2 #opencv library \n",
    "import argparse\n",
    "import imutils \n",
    "import sys \n",
    "import numpy as np #numpy library for calculations\n",
    "\n",
    "import pandas #pandas library to create csv files\n",
    "from datetime import datetime #datetime library to get date and time logs\n",
    "import os #os library to direct path of the training files\n",
    "import face_recognition #face-recognition module to specify face length\n",
    "\n",
    "#static variable to catch non-motion frame\n",
    "static_back = None\n",
    "\n",
    "#array to eppend frame when any moving object appear \n",
    "motion_list = [ None, None ]\n",
    "\n",
    "#time of movement \n",
    "time = [] \n",
    "\n",
    "#initializing DataFrame to ctach the moving object frame,\n",
    "#one column is start time and other column is end time \n",
    "df = pandas.DataFrame(columns = [\"Start\", \"End\"]) \n",
    "\n",
    "#use webcam for detection\n",
    "vidcap = cv2.VideoCapture(0)\n",
    "\n",
    "#to create background subtraction output window \n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "#define output video file for motion frames\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('G:\\output.avi',fourcc, 20.0, (640,480))\n",
    "\n",
    "#images' path to train for face detection and recognition\n",
    "path = (r'images\\Test')\n",
    "\n",
    "#empty arrays to append images\n",
    "images = []\n",
    "\n",
    "classNames = []\n",
    "\n",
    "myList = os.listdir(path)\n",
    "\n",
    "#print(myList)\n",
    "\n",
    "#read images and append them in array\n",
    "for cl in myList:\n",
    "    curImg = cv2.imread(f'{path}/{cl}')\n",
    "    images.append(curImg)\n",
    "    classNames.append(os.path.splitext(cl)[0])\n",
    "    \n",
    "#print(classNames)\n",
    "\n",
    "#function to encode images for recognition\n",
    "def findEncodings(images):\n",
    "    encodeList = []\n",
    "    for img in images:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        encode = face_recognition.face_encodings(img)[0]\n",
    "        encodeList.append(encode)\n",
    "    return encodeList\n",
    "\n",
    "#function to read recognized faces in a csv file \n",
    "def createlog(name):\n",
    "    with open(r'C:\\Users\\user\\Downloads\\log.csv','r+', errors = 'ignore') as f:\n",
    "        myDataList = f.readlines()\n",
    "        nameList = []\n",
    "        for line in myDataList:\n",
    "            entry = line.split(',')\n",
    "            nameList.append(entry[0])\n",
    "        if name not in nameList:\n",
    "            now = datetime.now()\n",
    "            dtString = now.strftime('%H:%M:%S')\n",
    "            f.writelines(f'\\n{name},{dtString}')\n",
    "\n",
    "#call encoded images function\n",
    "encodeListKnown = findEncodings(images)\n",
    "print('Encoding Complete')\n",
    "\n",
    "#read video frames for motion detection and face recognition\n",
    "ret, framePrimary = vidcap.read()\n",
    "ret, frameSecondary = vidcap.read()\n",
    "\n",
    "#empty array to count motion frames\n",
    "cnt = []\n",
    "\n",
    "#main function\n",
    "while vidcap.isOpened():\n",
    "    \n",
    "    #reading frame from video \n",
    "    check, framePrimary = vidcap.read() \n",
    "\n",
    "    #initializing motion = 0 (no motion) \n",
    "    motion = 0\n",
    "\n",
    "    #generate difference between two different output windows\n",
    "    difference = cv2.absdiff(framePrimary, frameSecondary)\n",
    "    \n",
    "    #conversion of RGB to grayscale\n",
    "    gray = cv2.cvtColor(difference, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #apply Gaussian blur to the frame\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    \n",
    "    #define threshold value using blur\n",
    "    _, thresh = cv2.threshold(blur, 20, 255, cv2.THRESH_BINARY)\n",
    "    dilated = cv2.dilate(thresh, None, iterations=3)\n",
    "    \n",
    "    #apply background subtractor for primary frame\n",
    "    fgmask = fgbg.apply(framePrimary)\n",
    "    \n",
    "    #establish contours \n",
    "    contours, _ = cv2.findContours(\n",
    "        dilated, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    blob = cv2.dnn.blobFromImages(framePrimary, 1.0,\n",
    "                                  (7, 7), (114, 107, 99),\n",
    "                                  swapRB=True, crop=True)\n",
    "    blob = np.transpose(blob, (1, 0, 2, 3))\n",
    "    blob = np.expand_dims(blob, axis=0)\n",
    "    contour_sum = 0 \n",
    "    \n",
    "    #read images from frames for face recognition\n",
    "    success, img = vidcap.read()\n",
    "    \n",
    "    #uncomment below to use screen as the primary source of live video input\n",
    "    #img = captureScreen()\n",
    "    \n",
    "    #resize images as per convenience   \n",
    "    imgS = cv2.resize(img,(0,0),None,0.25,0.25)\n",
    "    \n",
    "    #RGB encoding color to the images\n",
    "    imgS = cv2.cvtColor(imgS, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    #function for face recognition\n",
    "    facesCurFrame = face_recognition.face_locations(imgS)\n",
    "    encodesCurFrame = face_recognition.face_encodings(imgS,facesCurFrame)\n",
    "    \n",
    "    \n",
    "    #reading each frame for motion contour detection\n",
    "    for contour in contours:\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        \n",
    "        #specify contour area threshold value\n",
    "        if cv2.contourArea(contour) < 15000:\n",
    "            continue\n",
    "        \n",
    "        #motion = 1 for object in motion\n",
    "        motion = 1\n",
    "        \n",
    "        #to put a contour an text around object in motion\n",
    "        cv2.rectangle(framePrimary, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(framePrimary, \"Report: {}\".format('Movement'),\n",
    "                    (10, 18), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3)\n",
    "        cv2.putText(framePrimary,str(datetime.now()),(10, 400), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 3)\n",
    "        cnt.append(len(contours))\n",
    "        out.write(framePrimary)\n",
    "        cv2.putText(framePrimary,str(len(cnt)),(10, 180), cv2.FONT_HERSHEY_SIMPLEX, 4, (255, 255, 255), 4, cv2.LINE_AA)\n",
    "        \n",
    "        #for face recognition using encoded images\n",
    "        for encodeFace,faceLoc in zip(encodesCurFrame,facesCurFrame):\n",
    "            matches = face_recognition.compare_faces(encodeListKnown,encodeFace)\n",
    "            faceDis = face_recognition.face_distance(encodeListKnown,encodeFace)\n",
    "            \n",
    "            #print name of the face recognized\n",
    "            print(faceDis)\n",
    "            matchIndex = np.argmin(faceDis)\n",
    "            \n",
    "            #print the name of person recognized and time of recognition in a csv file\n",
    "            if matches[matchIndex]:\n",
    "                name = classNames[matchIndex].upper()\n",
    "                print(name)\n",
    "                #y1,x2,y2,x1 = faceLoc\n",
    "                #y1, x2, y2, x1 = y1*4,x2*4,y2*4,x1*4\n",
    "                #cv2.rectangle(img,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "                #cv2.rectangle(img,(x1,y2-35),(x2,y2),(0,255,0),cv2.FILLED)\n",
    "                #cv2.putText(img,name,(x1+6,y2-6),cv2.FONT_HERSHEY_COMPLEX,1,(255,255,255),2)\n",
    "                createlog(name)\n",
    "            \n",
    "    #appending the status of motion \n",
    "    motion_list.append(motion) \n",
    "    motion_list = motion_list[-2:] \n",
    "    \n",
    "    #appending start time of motion \n",
    "    if motion_list[-1] == 1 and motion_list[-2] == 0: \n",
    "        time.append(datetime.now())\n",
    "                \n",
    "    #appending End time of motion \n",
    "    if motion_list[-1] == 0 and motion_list[-2] == 1: \n",
    "        time.append(datetime.now()) \n",
    "         \n",
    "    #display both the output screen\n",
    "    cv2.imshow(\"feed\", framePrimary)\n",
    "    cv2.imshow(\"FG MASK Frame\", fgmask)\n",
    "\n",
    "    framePrimary = frameSecondary\n",
    "    ret, frameSecondary = vidcap.read()\n",
    "    \n",
    "    #generate key to quit the process\n",
    "    key = cv2.waitKey(1)\n",
    "    \n",
    "    #if q entered whole process will stop \n",
    "    if key == ord('q'): \n",
    "        if motion == 1: \n",
    "            time.append(datetime.now()) \n",
    "        break\n",
    "        \n",
    "#print final motion frames detected        \n",
    "print(len(cnt))\n",
    "\n",
    "#generate dataframe for start and end time for each motion frame\n",
    "for i in range(0, len(time), 2): \n",
    "    df = df.append({\"Start\":time[i], \"End\":time[i + 1]}, ignore_index = True) \n",
    "\n",
    "#creating a CSV file in which start and end time of movements will be saved \n",
    "df.to_csv(r\"G:\\time_of_movements.csv\") \n",
    "\n",
    "#destroy any opecv output window and stop kernel\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#release software resource\n",
    "vidcap.release()\n",
    "\n",
    "#generate output file of motion frames\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
